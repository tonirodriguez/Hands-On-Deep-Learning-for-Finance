{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras - Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading....  0 MMM\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  1 AXP\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  2 AMGN\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  3 AAPL\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  4 BA\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  5 CAT\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  6 CVX\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  7 CSCO\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  8 KO\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  9 DIS\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  10 GS\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  11 HD\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  12 HON\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  13 IBM\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  14 INTC\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  15 JNJ\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  16 JPM\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  17 MCD\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  18 MRK\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  19 MSFT\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  20 NKE\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  21 PG\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  22 CRM\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  23 TRV\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  24 UNH\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  25 VZ\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  26 V\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  27 WBA\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Downloading....  28 WMT\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "                  MMM        AXP       AMGN      AAPL         BA        CAT  \\\n",
      "Date                                                                          \n",
      "2008-03-19  54.271492  33.486897  30.908054  3.970519  54.094543  50.003811   \n",
      "2008-03-20  53.253952  36.660172  31.016310  4.080754  55.088787  50.064838   \n",
      "2008-03-24  53.704670  37.800331  31.689066  4.272434  56.186134  51.563671   \n",
      "2008-03-25  53.936882  37.880062  31.580809  4.316834  55.898914  51.977367   \n",
      "2008-03-26  53.718338  36.165848  32.477829  4.441763  56.193504  52.465675   \n",
      "\n",
      "                  CVX       CSCO         KO        DIS  ...       MSFT  \\\n",
      "Date                                                    ...              \n",
      "2008-03-19  47.751400  17.922314  19.519815  26.503473  ...  21.289881   \n",
      "2008-03-20  48.521114  18.142046  19.871408  27.063408  ...  21.706455   \n",
      "2008-03-24  48.987587  18.779243  19.916985  27.182182  ...  21.699007   \n",
      "2008-03-25  49.296650  18.859810  19.991865  27.216116  ...  21.676693   \n",
      "2008-03-26  49.541553  18.149364  19.907217  26.944633  ...  21.245247   \n",
      "\n",
      "                  NKE         PG      CRM        TRV        UNH         VZ  \\\n",
      "Date                                                                         \n",
      "2008-03-19  12.970128  44.842636  14.1525  32.930901  29.601980  16.877592   \n",
      "2008-03-20  14.111281  45.874271  13.9575  33.910091  29.410467  17.353228   \n",
      "2008-03-24  14.490969  46.271027  14.9100  33.839149  29.735218  17.761597   \n",
      "2008-03-25  14.342025  45.986687  15.2400  33.888809  29.377153  17.727961   \n",
      "2008-03-26  13.901507  46.052818  14.8100  33.739807  28.394587  17.362833   \n",
      "\n",
      "                    V        WBA        WMT  \n",
      "Date                                         \n",
      "2008-03-19  12.849034  26.163269  37.073318  \n",
      "2008-03-20  14.634252  26.450930  38.862011  \n",
      "2008-03-24  13.583586  27.767017  39.154041  \n",
      "2008-03-25  14.384093  27.335510  38.730598  \n",
      "2008-03-26  14.545559  27.299547  38.621082  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yfin\n",
    "\n",
    "yfin.pdr_override()\n",
    "\n",
    "# Dow Jones 30\n",
    "symbols_table = pd.read_html(\"https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average#Components\",header=0)[1]\n",
    "symbols = list(symbols_table.loc[:, \"Symbol\"])\n",
    "index_symbol = ['^DJI']\n",
    "\n",
    "\n",
    "# Dates\n",
    "start_date = '2008-01-01'\n",
    "end_date = '2017-12-31'\n",
    "\n",
    "\n",
    "# Download the data\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for i in range(len(symbols)):\n",
    "    symbols[i]=symbols[i].replace(u'\\xa0',u'').replace(\"NYSE:\",\"\")\n",
    "\n",
    "symbols.remove('DOW') # DOW data are unvailable on yahoo\n",
    "\n",
    "for i in range(len(symbols)):\n",
    "    print('Downloading.... ', i, symbols[i])\n",
    "\n",
    "    # User pandas_reader.data.DataReader to load the desired data. As simple as that.\n",
    "    data[symbols[i]] = pdr.get_data_yahoo(symbols[i], start_date, end_date)['Adj Close']\n",
    "    data_index = pdr.get_data_yahoo(index_symbol, start_date, end_date)['Adj Close']\n",
    "\n",
    "# Remove the missing the data from the dataframe\n",
    "data = data.dropna()\n",
    "data_index = data_index.dropna()\n",
    "\n",
    "# Save the data\n",
    "data.to_csv('dj30_10y.csv', sep=',', encoding='utf-8')\n",
    "data_index.to_csv('dj30_index_10y.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2465/2465 [==============================] - 2s 711us/step - loss: 0.0389\n",
      "Epoch 2/20\n",
      "2465/2465 [==============================] - 2s 894us/step - loss: 0.0132\n",
      "Epoch 3/20\n",
      "2465/2465 [==============================] - 2s 701us/step - loss: 0.0064\n",
      "Epoch 4/20\n",
      "2465/2465 [==============================] - 2s 661us/step - loss: 0.0044\n",
      "Epoch 5/20\n",
      "2465/2465 [==============================] - 2s 729us/step - loss: 0.0036\n",
      "Epoch 6/20\n",
      "2465/2465 [==============================] - 2s 643us/step - loss: 0.0032\n",
      "Epoch 7/20\n",
      "2465/2465 [==============================] - 2s 646us/step - loss: 0.0027\n",
      "Epoch 8/20\n",
      "2465/2465 [==============================] - 2s 720us/step - loss: 0.0024\n",
      "Epoch 9/20\n",
      "2465/2465 [==============================] - 2s 703us/step - loss: 0.0022\n",
      "Epoch 10/20\n",
      "2465/2465 [==============================] - 1s 596us/step - loss: 0.0022\n",
      "Epoch 11/20\n",
      "2465/2465 [==============================] - 2s 642us/step - loss: 0.0021\n",
      "Epoch 12/20\n",
      "2465/2465 [==============================] - 2s 792us/step - loss: 0.0020\n",
      "Epoch 13/20\n",
      "2465/2465 [==============================] - 2s 671us/step - loss: 0.0020\n",
      "Epoch 14/20\n",
      "2465/2465 [==============================] - 2s 694us/step - loss: 0.0019\n",
      "Epoch 15/20\n",
      "2465/2465 [==============================] - 2s 681us/step - loss: 0.0019\n",
      "Epoch 16/20\n",
      "2465/2465 [==============================] - 2s 757us/step - loss: 0.0019\n",
      "Epoch 17/20\n",
      "2465/2465 [==============================] - 2s 842us/step - loss: 0.0018\n",
      "Epoch 18/20\n",
      "2465/2465 [==============================] - 2s 744us/step - loss: 0.0019\n",
      "Epoch 19/20\n",
      "2465/2465 [==============================] - 2s 736us/step - loss: 0.0018\n",
      "Epoch 20/20\n",
      "2465/2465 [==============================] - 2s 673us/step - loss: 0.0018\n",
      "78/78 [==============================] - 0s 592us/step - loss: 0.0021\n",
      "test loss: 0.0021161523181945086\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('dj30_10y.csv', sep=',', engine='python')\n",
    "assets = data.columns.values[1:].tolist()\n",
    "data = data.iloc[:, 1:]\n",
    "\n",
    "# Load index\n",
    "index = pd.read_csv('dj30_index_10y.csv', sep=',', engine='python')\n",
    "index = index.iloc[-data.values.shape[0]:, 1:]\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler([0.1,0.9])\n",
    "data_X = scaler.fit_transform(data)\n",
    "scaler_index = MinMaxScaler([0.1,0.9])\n",
    "index = scaler_index.fit_transform(index)\n",
    "\n",
    "# Number of components\n",
    "N_COMPONENTS = 3\n",
    "\n",
    "## Autoencoder - Keras\n",
    "# Network hyperparameters\n",
    "n_inputs = len(assets)\n",
    "n_core = N_COMPONENTS\n",
    "n_outputs = n_inputs\n",
    "\n",
    "# Create model\n",
    "input = Input(shape=(n_inputs,))\n",
    "# Encoder\n",
    "encoded = Dense(n_core, activation='sigmoid')(input)\n",
    "# Decoder\n",
    "decoded = Dense(n_outputs, activation='sigmoid')(encoded)\n",
    "\n",
    "# define model\n",
    "autoencoder = Model(input, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Testing in-sample\n",
    "X_train = data_X\n",
    "X_test = data_X\n",
    "\n",
    "# Training parameters\n",
    "epochs = 20\n",
    "\n",
    "# Fit the model\n",
    "history = autoencoder.fit(X_train,\\\n",
    "                          X_train,\\\n",
    "                          epochs=epochs,\\\n",
    "                          batch_size=1,\\\n",
    "                          shuffle=True,\\\n",
    "                          verbose=1)\n",
    "\n",
    "# Make AE predictions\n",
    "y_pred_AE_keras = autoencoder.predict(X_test)\n",
    "\n",
    "print('test loss: '+str(autoencoder.evaluate(y_pred_AE_keras, X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow - Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/fastai/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "n_inputs = len(assets)\n",
    "n_core = N_COMPONENTS\n",
    "n_outputs = n_inputs\n",
    "\n",
    "initializer = tf.initializers.glorot_normal()\n",
    "w1 = tf.Variable(initializer([n_inputs, n_core]))\n",
    "w2 = tf.transpose(w1)\n",
    "b1 = tf.Variable(tf.zeros([n_core]))\n",
    "b2 = tf.Variable(tf.zeros([n_outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    return tf.nn.sigmoid(tf.add(tf.matmul(x, w1), b1))\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    return tf.nn.sigmoid(tf.add(tf.matmul(x, w2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 -> Loss: 0.001723\n",
      "Epoch: 1 -> Loss: 0.019032\n",
      "Epoch: 2 -> Loss: 0.003953\n",
      "Epoch: 3 -> Loss: 0.000775\n",
      "Epoch: 4 -> Loss: 0.000909\n",
      "Epoch: 5 -> Loss: 0.004756\n",
      "Epoch: 6 -> Loss: 0.002089\n",
      "Epoch: 7 -> Loss: 0.005746\n",
      "Epoch: 8 -> Loss: 0.002506\n",
      "Epoch: 9 -> Loss: 0.002924\n",
      "Epoch: 10 -> Loss: 0.001526\n",
      "Epoch: 11 -> Loss: 0.001658\n",
      "Epoch: 12 -> Loss: 0.001932\n",
      "Epoch: 13 -> Loss: 0.002264\n",
      "Epoch: 14 -> Loss: 0.003534\n",
      "Epoch: 15 -> Loss: 0.000872\n",
      "Epoch: 16 -> Loss: 0.003574\n",
      "Epoch: 17 -> Loss: 0.001141\n",
      "Epoch: 18 -> Loss: 0.001429\n",
      "Epoch: 19 -> Loss: 0.001274\n",
      "Test Error: 0.002522\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_inputs])\n",
    "Y = tf.placeholder(\"float\", [None, n_inputs])\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "y_true = X\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "mse = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(mse)\n",
    "\n",
    "# Start Training\n",
    "# Start a new TF session\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the network\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training\n",
    "    for i in range(epochs):\n",
    "        X_train1 = shuffle(X_train)\n",
    "        for j in range(X_train.shape[0] // batch_size):\n",
    "            batch_y = X_train1[j * batch_size:j * batch_size + batch_size, :]\n",
    "            batch_x = X_train1[j * batch_size:j * batch_size + batch_size, :]\n",
    "            _, loss_value = sess.run([optimizer, mse], feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "        # Display loss\n",
    "        print('Epoch: %i -> Loss: %f' % (i, loss_value))\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_AE_tf = sess.run(decoder_op, feed_dict={X: X_train, Y: X_train})\n",
    "    print('Test Error: %f' % tf.losses.mean_squared_error(X_train, y_pred_AE_tf).eval())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
