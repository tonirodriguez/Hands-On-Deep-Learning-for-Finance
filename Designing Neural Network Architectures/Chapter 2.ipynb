{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Layer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                384       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 929\n",
      "Trainable params: 929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "11/11 - 1s - loss: 0.5323 - accuracy: 0.8863\n",
      "Epoch 2/10\n",
      "11/11 - 0s - loss: 0.4260 - accuracy: 0.9173\n",
      "Epoch 3/10\n",
      "11/11 - 0s - loss: 0.3564 - accuracy: 0.9173\n",
      "Epoch 4/10\n",
      "11/11 - 0s - loss: 0.2994 - accuracy: 0.9202\n",
      "Epoch 5/10\n",
      "11/11 - 0s - loss: 0.2514 - accuracy: 0.9247\n",
      "Epoch 6/10\n",
      "11/11 - 0s - loss: 0.2116 - accuracy: 0.9380\n",
      "Epoch 7/10\n",
      "11/11 - 0s - loss: 0.1792 - accuracy: 0.9439\n",
      "Epoch 8/10\n",
      "11/11 - 0s - loss: 0.1519 - accuracy: 0.9601\n",
      "Epoch 9/10\n",
      "11/11 - 0s - loss: 0.1310 - accuracy: 0.9660\n",
      "Epoch 10/10\n",
      "11/11 - 0s - loss: 0.1139 - accuracy: 0.9675\n",
      "22/22 [==============================] - 0s 945us/step - loss: 0.1046 - accuracy: 0.9778\n",
      "Training Accuracy: 97.78%\n",
      "\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1354 - accuracy: 0.9670\n",
      "Testing Accuracy: 96.70%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = pd.read_csv('african_crises.csv')\n",
    "df = shuffle(df, random_state=11)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "# converting into useful numbers\n",
    "\n",
    "df['banking_crisis'] = df['banking_crisis'].replace('crisis',np.nan)\n",
    "df['banking_crisis'] = df['banking_crisis'].fillna(1)\n",
    "df['banking_crisis'] = df['banking_crisis'].replace('no_crisis',np.nan)\n",
    "df['banking_crisis'] = df['banking_crisis'].fillna(0)\n",
    "\n",
    "# removing unneccesary data\n",
    "\n",
    "df.drop(['cc3','country'], axis=1, inplace=True)\n",
    "\n",
    "# scaling the data\n",
    "\n",
    "df_scaled = preprocessing.scale(df)\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df.columns)\n",
    "df_scaled['banking_crisis'] = df['banking_crisis']\n",
    "df = df_scaled\n",
    "\n",
    "# defining the input data, X, and the desired results, y\n",
    "\n",
    "X = df.loc[:,df.columns != 'banking_crisis']\n",
    "y = df.loc[:, 'banking_crisis']\n",
    "\n",
    "# breaking data into training data, validation data, and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2,shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size = 0.2,shuffle=False)\n",
    "\n",
    "# constructing a Multilayer Perceptron\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32,activation = 'relu', input_dim = 11))\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy']) # adam, rmsprop, sgd\n",
    "\n",
    "model.summary()\n",
    "# training the network\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
    "\n",
    "# scoring it on the data it trained on as well as test data\n",
    "\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "print (\"Training Accuracy: %.2f%%\\n\" % (scores[1]*100))\n",
    "\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print (\"Testing Accuracy: %.2f%%\\n\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Layer Perceptron Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 8)                 112       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 18ms/step - loss: 22.0100 - mae: 22.0100 - val_loss: 23.1502 - val_mae: 23.1502\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 21.6590 - mae: 21.6590 - val_loss: 22.8761 - val_mae: 22.8761\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 21.2793 - mae: 21.2793 - val_loss: 22.5800 - val_mae: 22.5800\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 21.9182 - mae: 21.9182 - val_loss: 22.2422 - val_mae: 22.2422\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 20.1807 - mae: 20.1807 - val_loss: 21.8667 - val_mae: 21.8667\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 20.2946 - mae: 20.2946 - val_loss: 21.4522 - val_mae: 21.4522\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 19.1576 - mae: 19.1576 - val_loss: 20.9724 - val_mae: 20.9724\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 19.4603 - mae: 19.4603 - val_loss: 20.4262 - val_mae: 20.4262\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 18.9195 - mae: 18.9195 - val_loss: 19.8691 - val_mae: 19.8691\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 18.1415 - mae: 18.1415 - val_loss: 19.2565 - val_mae: 19.2565\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 16.9465 - mae: 16.9465 - val_loss: 18.6343 - val_mae: 18.6343\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 16.7193 - mae: 16.7193 - val_loss: 17.9093 - val_mae: 17.9093\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.8492 - mae: 15.8492 - val_loss: 17.1798 - val_mae: 17.1798\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 15.0191 - mae: 15.0191 - val_loss: 16.3503 - val_mae: 16.3503\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 14.4107 - mae: 14.4107 - val_loss: 15.5815 - val_mae: 15.5815\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 14.1297 - mae: 14.1297 - val_loss: 14.7631 - val_mae: 14.7631\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 12.2008 - mae: 12.2008 - val_loss: 13.9594 - val_mae: 13.9594\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 11.9679 - mae: 11.9679 - val_loss: 13.0329 - val_mae: 13.0329\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 11.0802 - mae: 11.0802 - val_loss: 12.1995 - val_mae: 12.1995\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 10.8968 - mae: 10.8968 - val_loss: 11.2718 - val_mae: 11.2718\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 9.6129 - mae: 9.6129 - val_loss: 10.4536 - val_mae: 10.4536\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 8.9741 - mae: 8.9741 - val_loss: 9.7892 - val_mae: 9.7892\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 8.8793 - mae: 8.8793 - val_loss: 9.2559 - val_mae: 9.2559\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 7.9187 - mae: 7.9187 - val_loss: 8.7376 - val_mae: 8.7376\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 7.5883 - mae: 7.5883 - val_loss: 8.2749 - val_mae: 8.2749\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 6.6380 - mae: 6.6380 - val_loss: 7.8243 - val_mae: 7.8243\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 6.5990 - mae: 6.5990 - val_loss: 7.5649 - val_mae: 7.5649\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.5382 - mae: 6.5382 - val_loss: 7.3498 - val_mae: 7.3498\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.3415 - mae: 6.3415 - val_loss: 7.1275 - val_mae: 7.1275\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.9834 - mae: 5.9834 - val_loss: 6.9541 - val_mae: 6.9541\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.8784 - mae: 5.8784 - val_loss: 6.7591 - val_mae: 6.7591\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.3460 - mae: 5.3460 - val_loss: 6.5744 - val_mae: 6.5744\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.0434 - mae: 5.0434 - val_loss: 6.3878 - val_mae: 6.3878\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.9300 - mae: 4.9300 - val_loss: 6.2167 - val_mae: 6.2167\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.9291 - mae: 4.9291 - val_loss: 6.1044 - val_mae: 6.1044\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 4.9324 - mae: 4.9324 - val_loss: 5.9631 - val_mae: 5.9631\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.8787 - mae: 4.8787 - val_loss: 5.8784 - val_mae: 5.8784\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 4.3507 - mae: 4.3507 - val_loss: 5.7498 - val_mae: 5.7498\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 4.6806 - mae: 4.6806 - val_loss: 5.6377 - val_mae: 5.6377\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 4.5229 - mae: 4.5229 - val_loss: 5.5743 - val_mae: 5.5743\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.5128 - mae: 4.5128 - val_loss: 5.4506 - val_mae: 5.4506\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.1433 - mae: 4.1433 - val_loss: 5.3717 - val_mae: 5.3717\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.4987 - mae: 4.4987 - val_loss: 5.2828 - val_mae: 5.2828\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.2875 - mae: 4.2875 - val_loss: 5.2150 - val_mae: 5.2150\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.1723 - mae: 4.1723 - val_loss: 5.1373 - val_mae: 5.1373\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.3433 - mae: 4.3433 - val_loss: 5.0759 - val_mae: 5.0759\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.2804 - mae: 4.2804 - val_loss: 4.9622 - val_mae: 4.9622\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.2817 - mae: 4.2817 - val_loss: 4.9285 - val_mae: 4.9285\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.2859 - mae: 4.2859 - val_loss: 4.8628 - val_mae: 4.8628\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.1648 - mae: 4.1648 - val_loss: 4.7820 - val_mae: 4.7820\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.9549 - mae: 3.9549 - val_loss: 4.7448 - val_mae: 4.7448\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.9011 - mae: 3.9011 - val_loss: 4.6983 - val_mae: 4.6983\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.9773 - mae: 3.9773 - val_loss: 4.6605 - val_mae: 4.6605\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.6084 - mae: 3.6084 - val_loss: 4.5878 - val_mae: 4.5878\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.8519 - mae: 3.8519 - val_loss: 4.5549 - val_mae: 4.5549\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.9649 - mae: 3.9649 - val_loss: 4.4761 - val_mae: 4.4761\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.6469 - mae: 3.6469 - val_loss: 4.4087 - val_mae: 4.4087\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.7080 - mae: 3.7080 - val_loss: 4.3563 - val_mae: 4.3563\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.9662 - mae: 3.9662 - val_loss: 4.3275 - val_mae: 4.3275\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.1596 - mae: 3.1596 - val_loss: 4.2747 - val_mae: 4.2747\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.3440 - mae: 3.3440 - val_loss: 4.2568 - val_mae: 4.2568\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.4725 - mae: 3.4725 - val_loss: 4.2323 - val_mae: 4.2323\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.3587 - mae: 3.3587 - val_loss: 4.1740 - val_mae: 4.1740\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.7148 - mae: 3.7148 - val_loss: 4.1054 - val_mae: 4.1054\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.3676 - mae: 3.3676 - val_loss: 4.0749 - val_mae: 4.0749\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.1205 - mae: 3.1205 - val_loss: 4.0494 - val_mae: 4.0494\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.2751 - mae: 3.2751 - val_loss: 3.9824 - val_mae: 3.9824\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.3649 - mae: 3.3649 - val_loss: 3.9667 - val_mae: 3.9667\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.4464 - mae: 3.4464 - val_loss: 3.9240 - val_mae: 3.9240\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.2298 - mae: 3.2298 - val_loss: 3.9163 - val_mae: 3.9163\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.2052 - mae: 3.2052 - val_loss: 3.8802 - val_mae: 3.8802\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.3879 - mae: 3.3879 - val_loss: 3.8335 - val_mae: 3.8335\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0666 - mae: 3.0666 - val_loss: 3.8093 - val_mae: 3.8093\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.3838 - mae: 3.3838 - val_loss: 3.7763 - val_mae: 3.7763\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.0372 - mae: 3.0372 - val_loss: 3.7758 - val_mae: 3.7758\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.9694 - mae: 2.9694 - val_loss: 3.7471 - val_mae: 3.7471\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.2425 - mae: 3.2425 - val_loss: 3.7278 - val_mae: 3.7278\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.4890 - mae: 3.4890 - val_loss: 3.7387 - val_mae: 3.7387\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.1132 - mae: 3.1132 - val_loss: 3.6871 - val_mae: 3.6871\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.9541 - mae: 2.9541 - val_loss: 3.7008 - val_mae: 3.7008\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.9610 - mae: 2.9610 - val_loss: 3.6796 - val_mae: 3.6796\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.9789 - mae: 2.9789 - val_loss: 3.6639 - val_mae: 3.6639\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.9808 - mae: 2.9808 - val_loss: 3.6390 - val_mae: 3.6390\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.1626 - mae: 3.1626 - val_loss: 3.6068 - val_mae: 3.6068\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.2186 - mae: 3.2186 - val_loss: 3.5894 - val_mae: 3.5894\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.8667 - mae: 2.8667 - val_loss: 3.5825 - val_mae: 3.5825\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.8659 - mae: 2.8659 - val_loss: 3.5905 - val_mae: 3.5905\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.0268 - mae: 3.0268 - val_loss: 3.5746 - val_mae: 3.5746\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.0593 - mae: 3.0593 - val_loss: 3.5541 - val_mae: 3.5541\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.0360 - mae: 3.0360 - val_loss: 3.5431 - val_mae: 3.5431\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.8216 - mae: 2.8216 - val_loss: 3.5353 - val_mae: 3.5353\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.9731 - mae: 2.9731 - val_loss: 3.5015 - val_mae: 3.5015\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0065 - mae: 3.0065 - val_loss: 3.4805 - val_mae: 3.4805\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.9921 - mae: 2.9921 - val_loss: 3.4946 - val_mae: 3.4946\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.6775 - mae: 2.6775 - val_loss: 3.4981 - val_mae: 3.4981\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.8175 - mae: 2.8175 - val_loss: 3.4690 - val_mae: 3.4690\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.0620 - mae: 3.0620 - val_loss: 3.4499 - val_mae: 3.4499\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 2.8182 - mae: 2.8182 - val_loss: 3.4398 - val_mae: 3.4398\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 2.9482 - mae: 2.9482 - val_loss: 3.3987 - val_mae: 3.3987\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 2.9158 - mae: 2.9158 - val_loss: 3.4033 - val_mae: 3.4033\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.6452 - mae: 3.6452\n",
      "[3.6451501846313477, 3.6451501846313477]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load dataset\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_shape=[X_train.shape[1]], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='rmsprop', metrics=['mae'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=100, verbose=1)\n",
    "\n",
    "print(model.evaluate(X_test_scaled, y_test))\n",
    "\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('model mean absolute error')\n",
    "plt.ylabel('mean absolute error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radial Basis Function Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4/4 [==============================] - 1s 2ms/step - loss: 0.7273 - accuracy: 0.1873\n",
      "Epoch 2/30\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7096 - accuracy: 0.3095\n",
      "Epoch 3/30\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6988 - accuracy: 0.4261\n",
      "Epoch 4/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6893 - accuracy: 0.6920\n",
      "Epoch 5/30\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6809 - accuracy: 0.8526\n",
      "Epoch 6/30\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6730 - accuracy: 0.8639\n",
      "Epoch 7/30\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6654 - accuracy: 0.8734\n",
      "Epoch 8/30\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6575 - accuracy: 0.9081\n",
      "Epoch 9/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6501 - accuracy: 0.9064\n",
      "Epoch 10/30\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6439 - accuracy: 0.9052\n",
      "Epoch 11/30\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.9082\n",
      "Epoch 12/30\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6313 - accuracy: 0.9094\n",
      "Epoch 13/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6253 - accuracy: 0.9065\n",
      "Epoch 14/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6153 - accuracy: 0.9141\n",
      "Epoch 15/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6117 - accuracy: 0.9069\n",
      "Epoch 16/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6061 - accuracy: 0.9091\n",
      "Epoch 17/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5985 - accuracy: 0.9087\n",
      "Epoch 18/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5908 - accuracy: 0.9102\n",
      "Epoch 19/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5877 - accuracy: 0.9103\n",
      "Epoch 20/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5855 - accuracy: 0.9057\n",
      "Epoch 21/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5771 - accuracy: 0.9077\n",
      "Epoch 22/30\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5744 - accuracy: 0.9089\n",
      "Epoch 23/30\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5694 - accuracy: 0.9072\n",
      "Epoch 24/30\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5596 - accuracy: 0.9107\n",
      "Epoch 25/30\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5587 - accuracy: 0.9096\n",
      "Epoch 26/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5491 - accuracy: 0.9078\n",
      "Epoch 27/30\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5460 - accuracy: 0.9124\n",
      "Epoch 28/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5474 - accuracy: 0.9007\n",
      "Epoch 29/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5388 - accuracy: 0.9111\n",
      "Epoch 30/30\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5367 - accuracy: 0.9093\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5319 - accuracy: 0.9079\n",
      "Training Accuracy: 90.79%\n",
      "\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.5490 - accuracy: 0.9245\n",
      "Testing Accuracy: 92.45%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class RBF(Layer):\n",
    "    def __init__(self, units, gamma, **kwargs):\n",
    "        super(RBF, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.gamma = K.cast_to_floatx(gamma)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mu = self.add_weight(name='mu',\n",
    "                                  shape=(int(input_shape[1]), self.units),\n",
    "                                  initializer='uniform',\n",
    "                                  trainable=True)\n",
    "        super(RBF, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        diff = K.expand_dims(inputs) - self.mu\n",
    "        l2 = K.sum(K.pow(diff, 2), axis=1)\n",
    "        res = K.exp(-1 * self.gamma * l2)\n",
    "        return res\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.units)\n",
    "\n",
    "\n",
    "df = pd.read_csv('african_crises.csv')\n",
    "df = shuffle(df, random_state=11)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "# converting into useful numbers\n",
    "\n",
    "df['banking_crisis'] = df['banking_crisis'].replace('crisis',np.nan)\n",
    "df['banking_crisis'] = df['banking_crisis'].fillna(1)\n",
    "df['banking_crisis'] = df['banking_crisis'].replace('no_crisis',np.nan)\n",
    "df['banking_crisis'] = df['banking_crisis'].fillna(0)\n",
    "\n",
    "# removing unneccesary data\n",
    "\n",
    "df.drop(['cc3','country'], axis=1, inplace=True)\n",
    "\n",
    "# scaling the data\n",
    "\n",
    "df_scaled = preprocessing.scale(df)\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df.columns)\n",
    "df_scaled['banking_crisis'] = df['banking_crisis']\n",
    "df = df_scaled\n",
    "\n",
    "# defining the input data, X, and the desired results, y\n",
    "\n",
    "X = df.loc[:,df.columns != 'banking_crisis']\n",
    "y = df.loc[:, 'banking_crisis']\n",
    "\n",
    "# breaking data into training data, validation data, and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2,shuffle=False)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "input = Input(shape=(X_train.shape[1],))\n",
    "rbf=RBF(10, 0.5)(input)\n",
    "out=Dense(1, activation='sigmoid')(rbf)\n",
    "\n",
    "model = Model(inputs=input, outputs=out)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss=binary_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=256, epochs=30)\n",
    "\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "print (\"Training Accuracy: %.2f%%\\n\" % (scores[1]*100))\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print (\"Testing Accuracy: %.2f%%\\n\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network - LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 10, 10, 16)        2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 120)               48120     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "\n",
    "# Instantiate an empty model\n",
    "model = Sequential()\n",
    "\n",
    "# C1 Convolutional Layer\n",
    "model.add(Conv2D(filters=6, kernel_size=5, strides=1, activation='relu', input_shape=(32, 32, 1), padding=\"valid\"))\n",
    "\n",
    "# S1 Pooling Layer\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# C2 Convolutional Layer\n",
    "model.add(Conv2D(filters=16, kernel_size=5, strides=1, activation='relu', input_shape=(14, 14, 6), padding=\"valid\"))\n",
    "\n",
    "# S2 Pooling Layer\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Flatten the output in order to feed the fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# FC1 Fully Connected Layer\n",
    "model.add(Dense(units=120, activation='relu'))\n",
    "\n",
    "# FC2 Fully Connected Layer\n",
    "model.add(Dense(units=84, activation='relu'))\n",
    "\n",
    "# FC3 Fully Connected Layer\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network - AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 54, 54, 96)        34944     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 27, 27, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 17, 17, 256)       2973952   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 384)         885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 6, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 2, 2, 256)         884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1, 1, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4096)              1052672   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 17)                17017     \n",
      "=================================================================\n",
      "Total params: 28,096,769\n",
      "Trainable params: 28,075,633\n",
      "Non-trainable params: 21,136\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=96, activation='relu',input_shape=(224, 224, 3), kernel_size=(11, 11), strides=(4, 4)))\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# Batch Normalisation before passing it to the next layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=256, activation='relu', kernel_size=(11, 11), strides=(1, 1)))\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=384, activation='relu', kernel_size=(3, 3), strides=(1, 1)))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, activation='relu', kernel_size=(3, 3), strides=(1, 1)))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, activation='relu', kernel_size=(3, 3), strides=(1, 1)))\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Passing it to a dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "# 1st Dense Layer\n",
    "model.add(Dense(4096, activation='relu', input_shape=(224*224*3,)))\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Dense Layer\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Dense Layer\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(17, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained networks - InceptionV3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96116736/96112376 [==============================] - 10s 0us/step\n",
      "Model: \"inception_v3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 111, 111, 32) 864         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 111, 111, 32) 96          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 111, 111, 32) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 109, 109, 32) 9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 109, 109, 32) 96          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 109, 109, 32) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 109, 109, 64) 18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 109, 109, 64) 192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 109, 109, 64) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 54, 54, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 54, 54, 80)   5120        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 54, 54, 80)   240         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 54, 54, 80)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 52, 52, 192)  138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 52, 52, 192)  576         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 52, 52, 192)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 25, 25, 64)   12288       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 25, 25, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 25, 25, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 25, 25, 48)   9216        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 25, 25, 96)   55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 25, 25, 48)   144         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 25, 25, 96)   288         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 25, 25, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 25, 25, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 25, 25, 192)  0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 25, 25, 64)   12288       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 25, 25, 64)   76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 25, 25, 96)   82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 25, 25, 32)   6144        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 25, 25, 64)   192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 25, 25, 64)   192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 25, 25, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 25, 25, 32)   96          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 25, 25, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 25, 25, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 25, 25, 96)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 25, 25, 32)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 25, 25, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 25, 25, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 25, 25, 96)   55296       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 25, 25, 48)   144         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 25, 25, 96)   288         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 25, 25, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 25, 25, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 25, 25, 64)   76800       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 25, 25, 96)   82944       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 25, 25, 64)   16384       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 25, 25, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 25, 25, 64)   192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 25, 25, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 25, 25, 64)   192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 25, 25, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 25, 25, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 25, 25, 96)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 25, 25, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 25, 25, 64)   192         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 25, 25, 64)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 25, 25, 96)   55296       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 25, 25, 48)   144         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 25, 25, 96)   288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 25, 25, 48)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 25, 25, 96)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 25, 25, 64)   76800       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 25, 25, 96)   82944       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 25, 25, 64)   18432       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 25, 25, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 25, 25, 64)   192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 25, 25, 96)   288         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 25, 25, 64)   192         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 25, 25, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 25, 25, 64)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 25, 25, 96)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 25, 25, 64)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 25, 25, 64)   192         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 25, 25, 64)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 25, 25, 96)   55296       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 25, 25, 96)   288         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 25, 25, 96)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 12, 12, 96)   82944       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 12, 12, 384)  1152        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 12, 12, 96)   288         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 12, 12, 384)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 12, 12, 96)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 12, 12, 128)  384         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 12, 12, 128)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 12, 12, 128)  114688      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 12, 12, 128)  384         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 12, 12, 128)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 12, 12, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 12, 12, 128)  384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 12, 12, 128)  384         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 12, 12, 128)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 12, 12, 128)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 12, 12, 128)  114688      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 12, 12, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 12, 12, 128)  384         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 12, 12, 128)  384         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 12, 12, 128)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 12, 12, 128)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 12, 12, 192)  172032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 12, 12, 192)  172032      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 12, 12, 192)  576         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 12, 12, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 12, 12, 192)  576         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 12, 12, 192)  576         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 12, 12, 192)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 12, 12, 192)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 12, 12, 192)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 12, 12, 192)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_30[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 12, 12, 160)  480         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 12, 12, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 12, 12, 160)  179200      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 12, 12, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 12, 12, 160)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 12, 12, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 12, 12, 160)  480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 12, 12, 160)  480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 12, 12, 160)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 12, 12, 160)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 12, 12, 160)  179200      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 12, 12, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 12, 12, 160)  480         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 12, 12, 160)  480         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 12, 12, 160)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 12, 12, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 12, 12, 192)  215040      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 12, 12, 192)  215040      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 12, 12, 192)  576         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 12, 12, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 12, 12, 192)  576         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 12, 12, 192)  576         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 12, 12, 192)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 12, 12, 192)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 12, 12, 192)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 12, 12, 192)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_40[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 12, 12, 160)  480         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 12, 12, 160)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 12, 12, 160)  179200      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 12, 12, 160)  480         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 12, 12, 160)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 12, 12, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 12, 12, 160)  480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 12, 12, 160)  480         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 12, 12, 160)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 12, 12, 160)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 12, 12, 160)  179200      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 12, 12, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 12, 12, 160)  480         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 12, 12, 160)  480         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 12, 12, 160)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 12, 12, 160)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 12, 12, 192)  215040      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 12, 12, 192)  215040      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 12, 12, 192)  576         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 12, 12, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 12, 12, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 12, 12, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 12, 12, 192)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 12, 12, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 12, 12, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 12, 12, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "                                                                 activation_58[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 12, 12, 192)  576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 12, 12, 192)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 12, 12, 192)  258048      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 12, 12, 192)  576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 12, 12, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 12, 12, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 12, 12, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 12, 12, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 12, 12, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 12, 12, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 12, 12, 192)  258048      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 12, 12, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 12, 12, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 12, 12, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 12, 12, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 12, 12, 192)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 12, 12, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 12, 12, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 12, 12, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 12, 12, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 12, 12, 192)  576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 12, 12, 192)  576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 12, 12, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 12, 12, 192)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 12, 12, 192)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 12, 12, 192)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_60[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 12, 12, 192)  576         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 12, 12, 192)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 12, 12, 192)  258048      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 12, 12, 192)  576         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 12, 12, 192)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 12, 12, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 12, 12, 192)  576         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 12, 12, 192)  576         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 12, 12, 192)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 12, 12, 192)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 5, 5, 320)    552960      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 5, 5, 192)    331776      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 5, 5, 320)    960         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 5, 5, 192)    576         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 5, 5, 320)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 5, 5, 192)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_71[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 5, 5, 448)    1344        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 5, 5, 448)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 5, 5, 384)    1548288     activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 5, 5, 384)    1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 5, 5, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 5, 5, 384)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 5, 5, 384)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 5, 5, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 5, 5, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 5, 5, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 5, 5, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 5, 5, 384)    1152        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 5, 5, 384)    1152        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 5, 5, 384)    1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 5, 5, 384)    1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 5, 5, 192)    245760      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 5, 5, 320)    960         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 5, 5, 384)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 5, 5, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 5, 5, 384)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 5, 5, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 5, 5, 192)    576         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 5, 5, 320)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_78[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 5, 5, 768)    0           activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 5, 5, 192)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_76[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 5, 5, 448)    1344        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 5, 5, 448)    0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 5, 5, 384)    1548288     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 5, 5, 384)    1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 5, 5, 384)    1152        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 5, 5, 384)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 5, 5, 384)    0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 5, 5, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 5, 5, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 5, 5, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 5, 5, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 5, 5, 384)    1152        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 5, 5, 384)    1152        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 5, 5, 384)    1152        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 5, 5, 384)    1152        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 5, 5, 192)    393216      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 5, 5, 320)    960         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 5, 5, 384)    0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 5, 5, 384)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 5, 5, 384)    0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 5, 5, 384)    0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 5, 5, 192)    576         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 5, 5, 320)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_87[0][0]              \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 5, 5, 768)    0           activation_91[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 5, 5, 192)    0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_85[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,851,784\n",
      "Trainable params: 23,817,352\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "model = InceptionV3(input_tensor=Input(shape=(224, 224, 3)) , weights='imagenet', include_top=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
